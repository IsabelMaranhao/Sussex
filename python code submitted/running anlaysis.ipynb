{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c23bfc-97e7-4811-92d2-d57e34a1bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs the windown analysis basic. look for 'running_window_analysis.ipynb' for better version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd96c0-3995-42f6-b94f-373ed9d383a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "#from dataClasses import Trial,Neuron\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "#import intertools \n",
    "# Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from imblearn.pipeline import  Pipeline\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ef4e1-9d81-4d38-8c79-bbe76ba9664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Defining the RNN layer\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        \n",
    "        # RNN forward pass\n",
    "        out, _ = self.rnn(x)\n",
    "        \n",
    "        # Use only the last time step's output\n",
    "        out = out[:, -1]  # Take the output from the last sequence step\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550951c-d0b3-430b-bd51-b856a50bd090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ready(brain_area, type_of_analysis):\n",
    "    all_trials=np.load(\"processed_data/\"+ brain_area + \"_all_trials.npy\",allow_pickle=True)\n",
    "    trial_list=np.empty((all_trials.shape[0],3))\n",
    "    trial_dff=np.empty((all_trials.shape[0],41)) \n",
    "    trial_licks=np.empty((all_trials.shape[0],41))\n",
    "    # Here we get the data based on the conditions that we want (i.e: if it is is lick, or go) \n",
    "    \n",
    "    for trial in range(all_trials.shape[0]):\n",
    "        \n",
    "        trial_list[trial,0]=all_trials[trial].neuron_num\n",
    "    \n",
    "        \n",
    "        if type_of_analysis==\"GO\":\n",
    "            # for labelling go nogo\n",
    "            if (all_trials[trial].trial_type)==\"go\":\n",
    "                trial_list[trial,1]=1\n",
    "            elif (all_trials[trial].trial_type)==\"nogo\":\n",
    "                trial_list[trial,1]=0\n",
    "            else:\n",
    "                print(\"gone wrong on neuron \",all_trials[trial].neuron_num )\n",
    "        elif type_of_analysis==\"LICK\":    \n",
    "            #for labelling lick nolick\n",
    "            if (all_trials[trial].trial_outcome)==\"FA\":\n",
    "                trial_list[trial,1]=1\n",
    "            elif (all_trials[trial].trial_outcome)==\"Hit\":\n",
    "                trial_list[trial,1]=1\n",
    "            elif (all_trials[trial].trial_outcome)==\"Miss\":\n",
    "                trial_list[trial,1]=0\n",
    "            elif (all_trials[trial].trial_outcome)==\"CR\":\n",
    "                trial_list[trial,1]=0\n",
    "            else:\n",
    "                print(\"gone wrong on neuron \",all_trials[trial].neuron_num )\n",
    "        else:\n",
    "            print(\"Type of analysis can only be 'GO' or 'LICK'\")\n",
    "        \n",
    "        trial_list[trial,2]=all_trials[trial].mouse_id\n",
    "        if (all_trials[trial].trial_outcome)==\"FA\" or (all_trials[trial].trial_outcome)==\"Hit\":\n",
    "            trial_dff[trial,0:41]=all_trials[trial].dff \n",
    "            #rebase  dff \n",
    "            trial_licks[trial,:]=all_trials[trial].licks\n",
    "            lick_start=np.argmax(trial_licks[trial,:]==1,axis=0)\n",
    "            trial_dff_rel_licks=np.zeros(trial_licks.shape[1])\n",
    "            new_start=lick_start-4\n",
    "            if new_start>=0:                    \n",
    "                # move the array of licks left so that timeseries starst at lick time\n",
    "                trial_dff_rel_licks[0:trial_licks.shape[1]-new_start]=all_trials[trial].dff[new_start: ]\n",
    "                ## need to mske the last few timesteps on the new series = final timestep on original series\n",
    "                trial_dff_rel_licks[trial_licks.shape[1]-new_start :]=all_trials[trial].dff[-1]\n",
    "            else: \n",
    "                # move the array of licks right so that timeseries starst at lick time\n",
    "                trial_dff_rel_licks[0-new_start :]=all_trials[trial].dff[0:all_trials[trial].dff.shape[0]+new_start]\n",
    "                ## need to mske the first few timesteps on the new series = first timestep on original series\n",
    "                trial_dff_rel_licks[0:-new_start]=all_trials[trial].dff[ 0 ]\n",
    "                #trial_dff[trial,41:82]=trial_dff_rel_licks\n",
    "        else:\n",
    "            trial_dff[trial,0:41]=all_trials[trial].dff \n",
    "            #trial_dff[trial,41:82]=all_trials[trial].dff\n",
    "    return trial_list, trial_dff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b43dd8-a556-4da9-bd92-d1b7f79e44ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6fec5a-4ba4-4608-8759-478371118e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the analysis for different windows for each neuron \n",
    "num_permutations = 1000\n",
    "results_window=[]\n",
    "# Brain areas and analysis types\n",
    "brain_area = (\"S\")\n",
    "type_of_analysis = \"GO\"\n",
    "\n",
    "for brain in brain_area:\n",
    "    trial_list, trial_dff = get_data_ready(\"S1naive\", type_of_analysis)\n",
    "    neuron_list = np.unique(trial_list[:, 0]).reshape(-1, 1)\n",
    "    \n",
    "    print(\"Num of neurons before filter=\", neuron_list.shape[0])\n",
    "    \n",
    "    #window_of_analysis=[\"REWARD\", \"STIM\"]\n",
    "    size_of_window=[1]\n",
    "    \n",
    "    for size in size_of_window:\n",
    "        window_start=np.arange(0,41,size)\n",
    "        for start in window_start: \n",
    "    \n",
    "            for neuron in neuron_list:\n",
    "                print(\"\\n\", neuron, \"XXXXXXXXXXXXXXXXXXXX\")\n",
    "                    \n",
    "                    # Extract neuron-specific data\n",
    "                neuron_data_list = trial_list[trial_list[:, 0] == neuron].copy()\n",
    "                neuron_data_dff = trial_dff[trial_list[:, 0] == neuron].copy()\n",
    "\n",
    "                # get window specific data\n",
    "                neuron_data_dff = neuron_data_dff[:,start:start+size] #smaller (all the way to 40)\n",
    "            \n",
    "                X = neuron_data_dff.copy()\n",
    "                y = neuron_data_list[:, 1]\n",
    "        \n",
    "               \n",
    "                # Train-test split\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "                # Apply PCA to reduce to 10 components\n",
    "                n_components=min(size,5)\n",
    "                pcaTrain = PCA(n_components=n_components)\n",
    "                pcaTest=PCA(n_components=n_components)\n",
    "                X_train_pca = pcaTrain.fit_transform(X_train)\n",
    "                X_test_pca = pcaTest.fit_transform(X_test)\n",
    "                \n",
    "                # Reshape PCA output to 3D for RNN input\n",
    "                # New shape: (num_samples,num_components,1)\n",
    "                num_components = X_train_pca.shape[1]\n",
    "                X_train_pca = X_train_pca.reshape(X_train_pca.shape[0], num_components, 1)\n",
    "                X_test_pca = X_test_pca.reshape(X_test_pca.shape[0], num_components, 1)\n",
    "    \n",
    "    \n",
    "                # Convert to PyTorch tensors\n",
    "                Xtrain = torch.from_numpy(X_train_pca).float()\n",
    "                ytrain = torch.from_numpy(y_train).long()\n",
    "                Xtest = torch.from_numpy(X_test_pca).float()\n",
    "                ytest = torch.from_numpy(y_test).long()\n",
    "        \n",
    "                # Batch size, epochs, and iterations\n",
    "                batch_size = 16\n",
    "                n_iters = 1000\n",
    "                num_epochs = int(n_iters // (len(X_train) // batch_size))\n",
    "        \n",
    "                train = TensorDataset(Xtrain, ytrain)\n",
    "                test = TensorDataset(Xtest, ytest)\n",
    "        \n",
    "                train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "                test_loader = DataLoader(test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        \n",
    "                # Create the RNN model\n",
    "                input_dim = X_train_pca.shape[-1]\n",
    "                hidden_dim = 4\n",
    "                layer_dim = 1\n",
    "                output_dim = 1\n",
    "        \n",
    "                model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "        \n",
    "                # Loss function and optimizer\n",
    "                error = nn.BCEWithLogitsLoss()  # No class weights, since oversampling handles imbalance\n",
    "                learning_rate = 0.01\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "        \n",
    "                # Training loop for the original model\n",
    "                train_accuracy_list = []\n",
    "                test_accuracy_list = []\n",
    "                test_loss_list = []\n",
    "        \n",
    "                for epoch in range(num_epochs):\n",
    "                    model.train()\n",
    "                    total_train, correct_train = 0, 0\n",
    "        \n",
    "                    for X_batch, y_batch in train_loader:\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(X_batch).squeeze()\n",
    "                        loss = error(outputs, y_batch.float())\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "        \n",
    "                        probabilities = torch.sigmoid(outputs)\n",
    "                        predicted = (probabilities > 0.5).long()\n",
    "                        correct_train += (predicted.squeeze() == y_batch).sum().item()\n",
    "                        total_train += y_batch.size(0)\n",
    "        \n",
    "                    train_accuracy = 100 * correct_train / total_train\n",
    "                    train_accuracy_list.append(train_accuracy)\n",
    "                    scheduler.step()\n",
    "        \n",
    "                    # Evaluate on test set\n",
    "                    model.eval()\n",
    "                    total_test, correct_test = 0, 0\n",
    "                    with torch.no_grad():\n",
    "                        for X_test_batch, y_test_batch in test_loader:\n",
    "                            outputs = model(X_test_batch).squeeze()\n",
    "                            probabilities = torch.sigmoid(outputs)\n",
    "                            predicted = (probabilities > 0.5).long()\n",
    "                            correct_test += (predicted == y_test_batch).sum().item()\n",
    "                            total_test += y_test_batch.size(0)\n",
    "                            loss_test = error(outputs, y_test_batch.float())\n",
    "                            test_loss_list.append(loss_test)\n",
    "        \n",
    "                    test_accuracy = 100 * correct_test / float(total_test)\n",
    "                    test_accuracy_list.append(test_accuracy)\n",
    "        \n",
    "                # Save the original model's test accuracy\n",
    "                original_test_accuracy = test_accuracy\n",
    "        \n",
    "                # Random classifier: Match class distribution in the test set\n",
    "                class_distribution = np.mean(y_test)  # proportion of class 1\n",
    "        \n",
    "                random_accuracies = []\n",
    "                for _ in range(num_permutations):\n",
    "                    y_random = np.random.choice([0, 1], size=len(y_test), p=[1 - class_distribution, class_distribution])\n",
    "                    random_accuracy = accuracy_score(y_test, y_random)\n",
    "                    random_accuracies.append(random_accuracy)\n",
    "        \n",
    "                # Calculate p-value using a t-test (testing original model vs. random classifiers)\n",
    "                random_accuracies = np.array(random_accuracies)\n",
    "                p_value = np.mean(random_accuracies >= original_test_accuracy/100)\n",
    "        \n",
    "                # Store results\n",
    "                results_window.append({\n",
    "                    'Brain Area': brain,\n",
    "                    'Window Size': size,\n",
    "                    'Window Start':start,\n",
    "                    'Neuron': neuron,\n",
    "                    'Train Accuracy': train_accuracy_list,\n",
    "                    'Test Accuracy': original_test_accuracy,\n",
    "                    'p value': p_value\n",
    "                })\n",
    "        \n",
    "                print(f\"Neuron {neuron}: Test Accuracy = {original_test_accuracy:.2f}%, p-value = {p_value:.4f}\")\n",
    "        \n",
    "                # Visualize random classifier accuracy distribution\n",
    "              #  plt.hist(random_accuracies, bins=20, color='blue', edgecolor='black')\n",
    "               # plt.axvline(original_test_accuracy/100, color='red', linestyle='dashed', linewidth=2)\n",
    "               # plt.title(f'Random Classifier Accuracy Distribution for Neuron {neuron}')\n",
    "               # plt.xlabel('Accuracy')\n",
    "               # plt.ylabel('Frequency')\n",
    "               # plt.show()\n",
    "        \n",
    "        \n",
    "             # Plot training vs test accuracy to examine potential overfitting or bias\n",
    "               # plt.plot(train_accuracy_list, label='Training Accuracy')\n",
    "                #plt.plot(test_accuracy_list, label='Test Accuracy')\n",
    "                #plt.xlabel('Epochs')\n",
    "                #plt.ylabel('Accuracy (%)')\n",
    "                #plt.title('Training vs Test Accuracy')\n",
    "                #plt.legend()\n",
    "                #plt.show()\n",
    "                \n",
    "                # Check class-specific accuracy on the test set\n",
    "                \n",
    "                # Predict on the test set and calculate confusion matrix\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(Xtest).squeeze()\n",
    "                    probabilities = torch.sigmoid(outputs)\n",
    "                    predictions = (probabilities > 0.5).long()\n",
    "                    \n",
    "                conf_matrix = confusion_matrix(ytest, predictions)\n",
    "                print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "                \n",
    "                # Calculate class-specific accuracy\n",
    "                class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "                print(\"Class-specific Accuracy:\", class_accuracy)\n",
    "\n",
    "                #save \n",
    "                results_df = pd.DataFrame(results_window)\n",
    "\n",
    "                # Save as CSV or Pickle\n",
    "                results_df.to_csv('results_window_s1naive.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2868b5-a58d-4ad4-82e1-4d8d135f7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results_window to a DataFrame\n",
    "results_df = pd.DataFrame(results_window)\n",
    "\n",
    "# Display the first few rows\n",
    "print(results_df.head())\n",
    "\n",
    "# Check for missing or unexpected values   \n",
    "print(results_df.info())\n",
    "print(results_df.describe())\n",
    "\n",
    "\n",
    "\n",
    "# Check unique values for grouping keys\n",
    "print(\"Unique brain areas:\", results_df['Brain Area'].unique())\n",
    "print(\"Unique window sizes:\", results_df['Window Size'].unique())\n",
    "print(\"Unique window starts:\", results_df['Window Start'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mouse)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
